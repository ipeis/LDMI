model:
  base_learning_rate: 2.0e-06
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    image_size: 4
    channels: 64
    monitor: val/loss_simple_ema

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 8              # spatial size of latent input
        in_channels: 32            # matches latent channel dim
        out_channels: 32           # same as in_channels
        model_channels: 128        # base hidden dim in UNet
        attention_resolutions: [1] # attention at 8x8
        num_res_blocks: 2
        channel_mult:
        - 1   # 8x8
        - 2   # 4x4
        num_head_channels: 64      # size of each attention head
    first_stage_config:
      target: ldm.models.autoencoder.IPVAE
      params:
        monitor: "val/rec_loss"
        ckpt_path: /dtu/p1/ipeaz/logs/LDMI/2025-07-12T08-40-05_chairs_ivae/checkpoints/last.ckpt
        lossconfig:
          target:  ldm.modules.losses.occupancy_loss.BernoulliLoss
          params:
            kl_weight: 1.0e-4
        encoder:
          target: ldm.modules.encoders.conv3d_encoder.Conv3DEncoder
          params:
            dim_z: 32
            base_channels: 64
            dropout: 0.1

        decoder:
          target: ldm.modules.decoders.trans_inr.TransInr
          params:
            data_shape: [32,32,32]
            update_strategy: scale
            tokenizer:
              target: ldm.modules.decoders.tokenizers.latent_tokenizer.LatentTokenizer
              params:                 # z is (latent_dim, latent_size, latent_size)
                latent_dim: 32        # latent depth is doubled in post_quant_conv
                latent_size: [8,8]    # 16 sized latent tensor  
                patch_size: 1         # Produces 64 tokens (8x8 grid)
                n_head: 4             # 4 attention heads for better feature mixing
                head_dim: 48     # ↑ from 32 → higher expressiveness

            inr:
              target: ldm.modules.decoders.inrs.siren.SIREN
              params:
                in_dim: 3
                out_dim: 1 
                out_bias: 0.5
                depth: 3
                hidden_dim: 128     # ↓ from 128 → faster convergence & fewer params

            n_groups: 64   # in Trans-INR CelebA: 64

            transformer:
              target: ldm.modules.decoders.transformer.Transformer
              params:
                dim: 64
                n_head: 4
                head_dim: 16
                ff_dim: 128
                encoder_depth: 4
                decoder_depth: 4
                dropout: 0.1


    cond_stage_config: __is_unconditional__

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 128
    num_workers: 8
    wrap: false
    train:
      target: ldm.data.pointcloud.VoxelTrain
      params:
        data_root: /work3/ipeaz/data/
    validation:
      target:  ldm.data.pointcloud.VoxelVal
      params:
        data_root: /work3/ipeaz/data/
    test:
      target:  ldm.data.pointcloud.VoxelTest
      params:
        data_root: /work3/ipeaz/data/

lightning:
  callbacks:
    image_logger:
      target: main.VoxelLogger
      params:
        batch_frequency: 10000
        max_images: 12
        cols: 6
        increase_log_steps: False
        clamp: True
        log_first_step: False
        log_images_kwargs:
          quantize_denoised: False
          inpaint: False
          plot_denoise_rows: False
          plot_progressive_rows: False
          plot_diffusion_rows: False

  trainer:
    max_epochs: -1  # Infinite epochs
    benchmark: True